{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 10:35:08 WARN Utils: Your hostname, MacBook-Pro-von-Philip.local resolves to a loopback address: 127.0.0.1; using 192.168.2.35 instead (on interface en0)\n",
      "24/02/22 10:35:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/22 10:35:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/22 10:35:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "#######################\n",
    "# Create Spark session\n",
    "#######################\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Trending Video Prediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load und Data prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+--------------------+--------------------+----------+-------------+--------------------+----------+-------------+-----------------+\n",
      "|   video_id|               title|        publishedAt|           channelId|        channelTitle|categoryId|trending_date|                tags|view_count|comment_count|comments_disabled|\n",
      "+-----------+--------------------+-------------------+--------------------+--------------------+----------+-------------+--------------------+----------+-------------+-----------------+\n",
      "|jhMP8RSv4ws|Final-Auftritt vo...|2024-02-17 22:40:00|UCvbR8mrSZ1BXf2LN...|     Das Supertalent|        24|     24.18.02|Supertalent|Das S...|    514610|          336|            False|\n",
      "|xeLMS48vrZU|Anna Ermakova - '...|2024-02-17 22:50:00|UCvbR8mrSZ1BXf2LN...|     Das Supertalent|        24|     24.18.02|Supertalent|Das S...|    487691|          332|            False|\n",
      "|CxpUiuI9O4s|Der unfassbare Fa...|2024-02-18 12:00:08|UCKGMHVipEvuZudhH...|      Simplicissimus|        24|     24.18.02|Simplicissimus|2 ...|    191084|          607|            False|\n",
      "|-ppK-fWTlPA|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|UCOnQ-e7kr4zKwR6R...|               CEDDO|        24|     24.18.02|ceddotalk|ceddo|b...|     78050|           95|            False|\n",
      "|nnOBbfVPRa8|Ich habe versucht...|2024-02-17 18:15:01|UC8E8eD7mOcnMazJT...|                Domo|        24|     24.18.02|Papaplatte|papapl...|    371820|          260|            False|\n",
      "|K9W12m6YGdE|Meine NERVEN LIEG...|2024-02-18 10:03:25|UCY7aQw00ds31nu29...|              Broski|        24|     24.18.02|gamerbrother|game...|    117771|          208|            False|\n",
      "|y95-aTC3HSU|Also wenn Skull a...|2024-02-17 17:00:28|UC6C1dyHHOMVIBAze...|            GameStar|        20|     24.18.02|spiele|game|gamep...|    447724|         2549|            False|\n",
      "|tKbObjo_68A|The FULL 2024 NBA...|2024-02-18 05:11:04|UCWJ2lWNubArHWmf3...|                 NBA|        17|     24.18.02|Basketball|G Leag...|   2336921|         4702|            False|\n",
      "|YhgCX2gTf7w|  ALMAN in der SAUNA|2024-02-18 11:00:17|UCwCpW0IbR0Z2WJry...|          Phil Laude|        24|     24.18.02|Alman|Alman vs Br...|    109911|          168|            False|\n",
      "|Onrjy0iGsPU|DIESES KREISLIGA ...|2024-02-18 12:00:29|UCgmHVWU9vo_Y4fiQ...|            Brotatos|        24|     24.18.02|fußball|challenge...|    161582|         1395|            False|\n",
      "|bFP9H0U1814|Die Aufrechten: D...|2024-02-17 08:00:14|UC1w6pNGiiLdZgyNp...|         DER SPIEGEL|        25|     24.18.02|Reportage|News|Po...|    585838|         3975|            False|\n",
      "|s3Vsvd9cuFQ|Isaak - Always On...|2024-02-17 01:05:55|UC238kiEEktbDUpIR...|Eurovision Song C...|        24|     24.18.02|Always On The Run...|    575468|         2141|            False|\n",
      "|AoLpWZPQuQ0|Isaak - Always On...|2024-02-16 23:06:53|UC238kiEEktbDUpIR...|Eurovision Song C...|        24|     24.18.02|Always On The Run...|    491572|         2532|            False|\n",
      "|-eqXQDLLEhc|Nur ein toter Bug...|2024-02-18 09:00:00|UC6C1dyHHOMVIBAze...|            GameStar|        20|     24.18.02|spiele|game|gamep...|    147093|          460|            False|\n",
      "|W0iQguIT_yE|MASSIVE ANNOUNCEM...|2024-02-17 18:13:43|UC4-79UOlP48-QNGg...|           MrBeast 2|        22|     24.18.02|              [none]|   3390103|        10505|            False|\n",
      "|-9a7awgvfuo|Worauf habt ihr g...|2024-02-16 18:00:07|UCpKgxZ4SgMKAgvQF...|           itsAssiTV|        20|     24.18.02|ps4|deutsch|Freun...|    263543|         2319|            False|\n",
      "|xQc5ea-cZrI|Unfassbar schlech...|2024-02-18 10:14:58|UCYAg4bYdyqENxEyH...|            der8auer|        28|     24.18.02|              [none]|     53240|          414|            False|\n",
      "|wujxPjysuAQ|Der unfairste Spe...|2024-02-18 12:15:02|UCMXLdjrQi4JzY0NN...|                Ulti|        20|     24.18.02|Yu-Gi-Oh!|Forbidd...|     65298|          214|            False|\n",
      "|whAvQZAili8|Patrick Cercel si...|2024-02-18 13:00:09|UCKQ3aENaIBeDGNz2...|Ionut Cercel © Of...|        10|     24.18.02|              [none]|    221252|         1119|            False|\n",
      "|k9nalORue4A|      Das Dorfleben.|2024-02-18 10:00:27|UCw81xLtFkpks4D8_...|              Varion|        23|     24.18.02|varion|fun|scetch...|     79555|          171|            False|\n",
      "+-----------+--------------------+-------------------+--------------------+--------------------+----------+-------------+--------------------+----------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Load the dataset\n",
    "###################################\n",
    "file_path = \"../ScraperData/DE.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "########################################\n",
    "# Show the first few rows of the dataset\n",
    "########################################\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedAt: timestamp (nullable = true)\n",
      " |-- channelId: string (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- categoryId: integer (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# Display the schema of the dataset to understand the data types\n",
    "################################################################\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedAt: timestamp (nullable = true)\n",
      " |-- channelId: string (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- categoryId: integer (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- comments_disabled: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Data Prep\n",
    "#################\n",
    "\n",
    "\n",
    "######################################\n",
    "# Change the Datatypes of the columns\n",
    "######################################\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Convert \"view_count\" column to integer type\n",
    "df = df.withColumn(\"view_count\", df[\"view_count\"].cast(\"int\"))\n",
    "# Convert \"comment_count\" column to integer type\n",
    "df = df.withColumn(\"comment_count\", df[\"comment_count\"].cast(\"int\"))\n",
    "# Convert \"categoryId\" column to integer type\n",
    "df = df.withColumn(\"categoryId\", df[\"categoryId\"].cast(\"int\"))\n",
    "# Convert \"comments_disabled\" column to boolean type\n",
    "df = df.withColumn(\"comments_disabled\", when(df[\"comments_disabled\"] == \"True\", True).otherwise(False))\n",
    "\n",
    "\n",
    "# Show the DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# Load JSON file and convert it to a DataFrame\n",
    "##############################################\n",
    "\n",
    "import json\n",
    "\n",
    "category_id_df = spark.read.json('../ScraperData/DE_category_id.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_id_df.printSchema()\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Sub-Spark-Session for Handling Corrupt Error\n",
    "###############################################\n",
    "\n",
    "\n",
    "#Sucessfully solved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|id |title               |\n",
      "+---+--------------------+\n",
      "|1  |Film & Animation    |\n",
      "|2  |Autos & Vehicles    |\n",
      "|10 |Music               |\n",
      "|15 |Pets & Animals      |\n",
      "|17 |Sports              |\n",
      "|18 |Short Movies        |\n",
      "|19 |Travel & Events     |\n",
      "|20 |Gaming              |\n",
      "|21 |Videoblogging       |\n",
      "|22 |People & Blogs      |\n",
      "|23 |Comedy              |\n",
      "|24 |Entertainment       |\n",
      "|25 |News & Politics     |\n",
      "|26 |Howto & Style       |\n",
      "|27 |Education           |\n",
      "|28 |Science & Technology|\n",
      "|30 |Movies              |\n",
      "|31 |Anime/Animation     |\n",
      "|32 |Action/Adventure    |\n",
      "|33 |Classics            |\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Load Json File and prepare for merging with the DE_csv \n",
    "########################################################\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "\n",
    "################################################\n",
    "# Schema der json definieren\n",
    "################################################\n",
    "schema = StructType([\n",
    "    StructField(\"kind\", StringType(), True),\n",
    "    StructField(\"etag\", StringType(), True),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"snippet\", StructType([\n",
    "            StructField(\"title\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True), True)\n",
    "])\n",
    "\n",
    "#################\n",
    "# Load Json\n",
    "#################\n",
    "df_test = spark.read.schema(schema) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json('../ScraperData/DE_category_id.json')\n",
    "\n",
    "##########################################\n",
    "# Select and transform to get id and title\n",
    "##########################################\n",
    "df_transformed = df_test.select(explode(\"items\").alias(\"item\")).select(\n",
    "    col(\"item.id\").alias(\"id\"),\n",
    "    col(\"item.snippet.title\").alias(\"title\")\n",
    ")\n",
    "\n",
    "################################\n",
    "# Show the transformed dataframe\n",
    "################################\n",
    "df_transformed.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|       categoryTitle|\n",
      "+---+--------------------+\n",
      "|  1|    Film & Animation|\n",
      "|  2|    Autos & Vehicles|\n",
      "| 10|               Music|\n",
      "| 15|      Pets & Animals|\n",
      "| 17|              Sports|\n",
      "| 18|        Short Movies|\n",
      "| 19|     Travel & Events|\n",
      "| 20|              Gaming|\n",
      "| 21|       Videoblogging|\n",
      "| 22|      People & Blogs|\n",
      "| 23|              Comedy|\n",
      "| 24|       Entertainment|\n",
      "| 25|     News & Politics|\n",
      "| 26|       Howto & Style|\n",
      "| 27|           Education|\n",
      "| 28|Science & Technology|\n",
      "| 30|              Movies|\n",
      "| 31|     Anime/Animation|\n",
      "| 32|    Action/Adventure|\n",
      "| 33|            Classics|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# Titel zu CategoryTitel ändern, dass keine doppelten Columns im Join-Prozess sind\n",
    "##################################################################################\n",
    "# Rename the 'title' column to 'Filmtitel'\n",
    "df_transformed_renamed = df_transformed.withColumnRenamed(\"title\", \"categoryTitle\")\n",
    "\n",
    "# Show the DataFrame to verify the change\n",
    "df_transformed_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Save the transformed DataFrame to a new JSON file\n",
    "###################################################\n",
    "df_transformed_renamed.write.mode(\"overwrite\").json(\"../ScraperData/cleanDEcategory.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_id', 'title', 'publishedAt', 'channelId', 'channelTitle', 'categoryId', 'trending_date', 'tags', 'view_count', 'comment_count', 'comments_disabled']\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Print the column names of df\n",
    "##############################\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+--------------------+--------------------+----------+-------------+--------------------+----------+-------------+-----------------+--------------------+\n",
      "|   video_id|               title|        publishedAt|           channelId|        channelTitle|categoryId|trending_date|                tags|view_count|comment_count|comments_disabled|       categoryTitle|\n",
      "+-----------+--------------------+-------------------+--------------------+--------------------+----------+-------------+--------------------+----------+-------------+-----------------+--------------------+\n",
      "|jhMP8RSv4ws|Final-Auftritt vo...|2024-02-17 22:40:00|UCvbR8mrSZ1BXf2LN...|     Das Supertalent|        24|     24.18.02|Supertalent|Das S...|    514610|          336|            false|       Entertainment|\n",
      "|xeLMS48vrZU|Anna Ermakova - '...|2024-02-17 22:50:00|UCvbR8mrSZ1BXf2LN...|     Das Supertalent|        24|     24.18.02|Supertalent|Das S...|    487691|          332|            false|       Entertainment|\n",
      "|CxpUiuI9O4s|Der unfassbare Fa...|2024-02-18 12:00:08|UCKGMHVipEvuZudhH...|      Simplicissimus|        24|     24.18.02|Simplicissimus|2 ...|    191084|          607|            false|       Entertainment|\n",
      "|-ppK-fWTlPA|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|UCOnQ-e7kr4zKwR6R...|               CEDDO|        24|     24.18.02|ceddotalk|ceddo|b...|     78050|           95|            false|       Entertainment|\n",
      "|nnOBbfVPRa8|Ich habe versucht...|2024-02-17 18:15:01|UC8E8eD7mOcnMazJT...|                Domo|        24|     24.18.02|Papaplatte|papapl...|    371820|          260|            false|       Entertainment|\n",
      "|K9W12m6YGdE|Meine NERVEN LIEG...|2024-02-18 10:03:25|UCY7aQw00ds31nu29...|              Broski|        24|     24.18.02|gamerbrother|game...|    117771|          208|            false|       Entertainment|\n",
      "|y95-aTC3HSU|Also wenn Skull a...|2024-02-17 17:00:28|UC6C1dyHHOMVIBAze...|            GameStar|        20|     24.18.02|spiele|game|gamep...|    447724|         2549|            false|              Gaming|\n",
      "|tKbObjo_68A|The FULL 2024 NBA...|2024-02-18 05:11:04|UCWJ2lWNubArHWmf3...|                 NBA|        17|     24.18.02|Basketball|G Leag...|   2336921|         4702|            false|              Sports|\n",
      "|YhgCX2gTf7w|  ALMAN in der SAUNA|2024-02-18 11:00:17|UCwCpW0IbR0Z2WJry...|          Phil Laude|        24|     24.18.02|Alman|Alman vs Br...|    109911|          168|            false|       Entertainment|\n",
      "|Onrjy0iGsPU|DIESES KREISLIGA ...|2024-02-18 12:00:29|UCgmHVWU9vo_Y4fiQ...|            Brotatos|        24|     24.18.02|fußball|challenge...|    161582|         1395|            false|       Entertainment|\n",
      "|bFP9H0U1814|Die Aufrechten: D...|2024-02-17 08:00:14|UC1w6pNGiiLdZgyNp...|         DER SPIEGEL|        25|     24.18.02|Reportage|News|Po...|    585838|         3975|            false|     News & Politics|\n",
      "|s3Vsvd9cuFQ|Isaak - Always On...|2024-02-17 01:05:55|UC238kiEEktbDUpIR...|Eurovision Song C...|        24|     24.18.02|Always On The Run...|    575468|         2141|            false|       Entertainment|\n",
      "|AoLpWZPQuQ0|Isaak - Always On...|2024-02-16 23:06:53|UC238kiEEktbDUpIR...|Eurovision Song C...|        24|     24.18.02|Always On The Run...|    491572|         2532|            false|       Entertainment|\n",
      "|-eqXQDLLEhc|Nur ein toter Bug...|2024-02-18 09:00:00|UC6C1dyHHOMVIBAze...|            GameStar|        20|     24.18.02|spiele|game|gamep...|    147093|          460|            false|              Gaming|\n",
      "|W0iQguIT_yE|MASSIVE ANNOUNCEM...|2024-02-17 18:13:43|UC4-79UOlP48-QNGg...|           MrBeast 2|        22|     24.18.02|              [none]|   3390103|        10505|            false|      People & Blogs|\n",
      "|-9a7awgvfuo|Worauf habt ihr g...|2024-02-16 18:00:07|UCpKgxZ4SgMKAgvQF...|           itsAssiTV|        20|     24.18.02|ps4|deutsch|Freun...|    263543|         2319|            false|              Gaming|\n",
      "|xQc5ea-cZrI|Unfassbar schlech...|2024-02-18 10:14:58|UCYAg4bYdyqENxEyH...|            der8auer|        28|     24.18.02|              [none]|     53240|          414|            false|Science & Technology|\n",
      "|wujxPjysuAQ|Der unfairste Spe...|2024-02-18 12:15:02|UCMXLdjrQi4JzY0NN...|                Ulti|        20|     24.18.02|Yu-Gi-Oh!|Forbidd...|     65298|          214|            false|              Gaming|\n",
      "|whAvQZAili8|Patrick Cercel si...|2024-02-18 13:00:09|UCKQ3aENaIBeDGNz2...|Ionut Cercel © Of...|        10|     24.18.02|              [none]|    221252|         1119|            false|               Music|\n",
      "|k9nalORue4A|      Das Dorfleben.|2024-02-18 10:00:27|UCw81xLtFkpks4D8_...|              Varion|        23|     24.18.02|varion|fun|scetch...|     79555|          171|            false|              Comedy|\n",
      "+-----------+--------------------+-------------------+--------------------+--------------------+----------+-------------+--------------------+----------+-------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# Merge CSV und Json mit category\n",
    "###################################################################\n",
    "df_joined = df.join(df_transformed_renamed, df.categoryId == df_transformed_renamed.id, 'left') \\\n",
    "               #.withColumnRenamed(\"title\", \"category\")\n",
    "\n",
    "df_final = df_joined.drop(\"id\")\n",
    "\n",
    "\n",
    "df_final.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedAt: timestamp (nullable = true)\n",
      " |-- channelId: string (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- categoryId: integer (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- comments_disabled: boolean (nullable = false)\n",
      " |-- categoryTitle: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Print the schema of the dataset\n",
    "#################################\n",
    "df_final.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Entferne Spalten, die wir nicht nutzen werden für die Models etc.\n",
    "####################################################################\n",
    "\n",
    "# Zeiten, für andere Model wichtig, wenn Zeit von Realease des Videos bis es in den Trends erscheint wichtig ist. || Oder, wenn das Datum wichtig ist zu der ein bestimmtes Video trendet, bzw. der Zeituraum\n",
    "#\"publishedAt\",\"trending_date\"\n",
    "df = df_final.drop(\"video_id\", \"categoryId\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+---------------+-------------+--------------------+----------+-------------+-----------------+-------------+\n",
      "|               title|        publishedAt|           channelId|   channelTitle|trending_date|                tags|view_count|comment_count|comments_disabled|categoryTitle|\n",
      "+--------------------+-------------------+--------------------+---------------+-------------+--------------------+----------+-------------+-----------------+-------------+\n",
      "|Final-Auftritt vo...|2024-02-17 22:40:00|UCvbR8mrSZ1BXf2LN...|Das Supertalent|     24.18.02|Supertalent|Das S...|    514610|          336|            false|Entertainment|\n",
      "|Anna Ermakova - '...|2024-02-17 22:50:00|UCvbR8mrSZ1BXf2LN...|Das Supertalent|     24.18.02|Supertalent|Das S...|    487691|          332|            false|Entertainment|\n",
      "+--------------------+-------------------+--------------------+---------------+-------------+--------------------+----------+-------------+-----------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# Check, ob ein Video Comments deaktiviert hat\n",
    "##############################################\n",
    "\n",
    "df_true_check = df.filter(df.comments_disabled == True)\n",
    "print(df_true_check.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Entfernen weiterer Spalten\n",
    "####################################\n",
    "\n",
    "df = df.drop(\"channelId\",\"comments_disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedAt: timestamp (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- categoryTitle: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+\n",
      "|               title|        publishedAt|        channelTitle|trending_date|                tags|view_count|comment_count|  categoryTitle|is_trending|\n",
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+\n",
      "|Final-Auftritt vo...|2024-02-17 22:40:00|     Das Supertalent|     24.18.02|Supertalent|Das S...|    514610|          336|  Entertainment|          0|\n",
      "|Anna Ermakova - '...|2024-02-17 22:50:00|     Das Supertalent|     24.18.02|Supertalent|Das S...|    487691|          332|  Entertainment|          0|\n",
      "|Der unfassbare Fa...|2024-02-18 12:00:08|      Simplicissimus|     24.18.02|Simplicissimus|2 ...|    191084|          607|  Entertainment|          0|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.18.02|ceddotalk|ceddo|b...|     78050|           95|  Entertainment|          0|\n",
      "|Ich habe versucht...|2024-02-17 18:15:01|                Domo|     24.18.02|Papaplatte|papapl...|    371820|          260|  Entertainment|          0|\n",
      "|Meine NERVEN LIEG...|2024-02-18 10:03:25|              Broski|     24.18.02|gamerbrother|game...|    117771|          208|  Entertainment|          0|\n",
      "|Also wenn Skull a...|2024-02-17 17:00:28|            GameStar|     24.18.02|spiele|game|gamep...|    447724|         2549|         Gaming|          0|\n",
      "|The FULL 2024 NBA...|2024-02-18 05:11:04|                 NBA|     24.18.02|Basketball|G Leag...|   2336921|         4702|         Sports|          1|\n",
      "|  ALMAN in der SAUNA|2024-02-18 11:00:17|          Phil Laude|     24.18.02|Alman|Alman vs Br...|    109911|          168|  Entertainment|          0|\n",
      "|DIESES KREISLIGA ...|2024-02-18 12:00:29|            Brotatos|     24.18.02|fußball|challenge...|    161582|         1395|  Entertainment|          0|\n",
      "|Die Aufrechten: D...|2024-02-17 08:00:14|         DER SPIEGEL|     24.18.02|Reportage|News|Po...|    585838|         3975|News & Politics|          1|\n",
      "|Isaak - Always On...|2024-02-17 01:05:55|Eurovision Song C...|     24.18.02|Always On The Run...|    575468|         2141|  Entertainment|          1|\n",
      "|Isaak - Always On...|2024-02-16 23:06:53|Eurovision Song C...|     24.18.02|Always On The Run...|    491572|         2532|  Entertainment|          0|\n",
      "|Nur ein toter Bug...|2024-02-18 09:00:00|            GameStar|     24.18.02|spiele|game|gamep...|    147093|          460|         Gaming|          0|\n",
      "|MASSIVE ANNOUNCEM...|2024-02-17 18:13:43|           MrBeast 2|     24.18.02|              [none]|   3390103|        10505| People & Blogs|          1|\n",
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Festlegen ab wann ein Video als trending für das Model zählt\n",
    "##############################################################\n",
    "df = df.withColumn('is_trending', when((col('view_count') > 500000) & (col('comment_count') > 1000), 1).otherwise(0))\n",
    "\n",
    "\n",
    "df.show(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 1000 rows.\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Scraper druckt pro Tag 200 neue Entries\n",
    "#########################################\n",
    "row_count = df.count()\n",
    "\n",
    "print(f\"The DataFrame has {row_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Übersicht der Anzahl trendender Videos in einer Kategorie\n",
    "###########################################################\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "trending_videos_df = df.filter(df.is_trending == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|       categoryTitle|trending_count|\n",
      "+--------------------+--------------+\n",
      "|               Music|            69|\n",
      "|              Sports|            57|\n",
      "|       Entertainment|            38|\n",
      "|              Gaming|            33|\n",
      "|      People & Blogs|            23|\n",
      "|              Comedy|            15|\n",
      "|Science & Technology|            13|\n",
      "|    Film & Animation|             9|\n",
      "|    Autos & Vehicles|             8|\n",
      "|     News & Politics|             5|\n",
      "|           Education|             2|\n",
      "|       Howto & Style|             2|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trending_count_by_category = trending_videos_df.groupBy(\"categoryTitle\").agg(count(\"is_trending\").alias(\"trending_count\"))\n",
    "trending_count_by_category = trending_count_by_category.orderBy(\"trending_count\", ascending=False)\n",
    "\n",
    "trending_count_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "#######################################\n",
    "# One-Hot-Encoding\n",
    "#######################################\n",
    "indexer = StringIndexer(inputCol=\"categoryTitle\", outputCol=\"category_index\").fit(df)\n",
    "encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_vec\")\n",
    "\n",
    "###########################\n",
    "# Assemble features\n",
    "###########################\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"category_vec\", \"view_count\", \"comment_count\"], outputCol=\"features\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "##########################\n",
    "# Pipeline Rahmen setzen\n",
    "##########################\n",
    "pipeline = Pipeline(stages=[indexer,encoder,assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "# Fit Pipline to Data\n",
    "########################\n",
    "pipline_model = pipeline.fit(df)\n",
    "\n",
    "###############################\n",
    "# Transforms the Data\n",
    "###############################\n",
    "df_transformed = pipline_model.transform(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|is_trending|\n",
      "+--------------------+-----------+\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[3,12,13],[1....|          0|\n",
      "|(14,[1,12,13],[1....|          1|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[5,12,13],[1....|          1|\n",
      "|(14,[0,12,13],[1....|          1|\n",
      "|(14,[0,12,13],[1....|          0|\n",
      "|(14,[3,12,13],[1....|          0|\n",
      "|(14,[4,12,13],[1....|          1|\n",
      "|(14,[3,12,13],[1....|          0|\n",
      "|(14,[6,12,13],[1....|          0|\n",
      "|(14,[3,12,13],[1....|          0|\n",
      "|(14,[2,12,13],[1....|          0|\n",
      "|(14,[7,12,13],[1....|          0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_transformed.select(\"features\", \"is_trending\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Splitting the data into Trainingsdata and Testdata\n",
    "####################################################\n",
    "(train_data, test_data) = df_transformed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Initialize the Random Forest model\n",
    "#####################################\n",
    "rf = RandomForestClassifier(labelCol=\"is_trending\", featuresCol=\"features\")\n",
    "\n",
    "#####################################\n",
    "# Initialize evaluator\n",
    "#####################################\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_trending\")\n",
    "\n",
    "#####################\n",
    "# Parameter setzen\n",
    "#####################\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [5, 10, 20])\n",
    "             .addGrid(rf.numTrees, [20, 50, 100])\n",
    "             .build())\n",
    "\n",
    "\n",
    "#################################################\n",
    "# Cross Validator setzen und bestes Model finden\n",
    "#################################################\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cv_model = cv.fit(train_data)\n",
    "bestModel = cv_model.bestModel\n",
    "best_model_predictions = bestModel.transform(test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------+--------------------+----------+-------------+-------------+-----------+--------------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|               title|        publishedAt|       channelTitle|trending_date|                tags|view_count|comment_count|categoryTitle|is_trending|category_index|  category_vec|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------------+-------------------+-------------+--------------------+----------+-------------+-------------+-----------+--------------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|1. FC Heidenheim ...|2024-02-19 00:00:16|sportstudio fußball|     24.20.02|Fußball|Fussball|...|    460727|          319|       Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|[19.5624549049290...|[0.97812274524645...|       0.0|\n",
      "+--------------------+-------------------+-------------------+-------------+--------------------+----------+-------------+-------------+-----------+--------------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_predictions.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.9988359911535328\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Evaluate the best model\n",
    "##########################\n",
    "best_model_accuracy = evaluator.evaluate(best_model_predictions)\n",
    "print(f\"Best Model Accuracy: {best_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+--------------+--------------+--------------------+\n",
      "|               title|        publishedAt|        channelTitle|trending_date|                tags|view_count|comment_count|  categoryTitle|is_trending|category_index|  category_vec|            features|\n",
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+--------------+--------------+--------------------+\n",
      "|1. FC Heidenheim ...|2024-02-19 00:00:16| sportstudio fußball|     24.20.02|Fußball|Fussball|...|    460727|          319|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|1. FC Heidenheim:...|2024-02-19 16:00:08|         Manu Thiele|     24.21.02|Fußball|Fußball N...|     88738|          493|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|1. FC Heidenheim:...|2024-02-19 16:00:08|         Manu Thiele|     24.21.02|Fußball|Fußball N...|     91215|          501|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|10 Minute Evening...|2024-02-18 08:00:15|       Mady Morrison|     24.18.02|mady morrison|str...|     28422|          109|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|10 Minute Evening...|2024-02-18 08:00:15|       Mady Morrison|     24.21.02|mady morrison|str...|    132518|          172|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|10 Things Conor M...|2024-02-14 18:00:04|           GQ Sports|     24.20.02|10 essentials|bla...|   1757681|         2568|         Sports|          1|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|17 Minuten extrem...|2024-02-18 12:00:26|             zCrexpy|     24.21.02|papaplatte|papapl...|    156535|           90|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|24 Stunden Fußbal...|2024-02-15 15:00:42|Das schaffst du nie!|     24.18.02|Das schaffst du n...|    235669|          930|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|24 Stunden Fußbal...|2024-02-15 15:00:42|Das schaffst du nie!|     24.21.02|Das schaffst du n...|    271270|          992|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|5. Mio € Deal & M...|2024-02-18 20:00:38|                Alim|     24.20.02|              [none]|    188571|          526|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.18.02|versus 1|versus 1...|   1446676|         4713|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.20.02|versus 1|versus 1...|   1745302|         5299|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.21.02|versus 1|versus 1...|   1849125|         5461|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.21.02|versus 1|versus 1...|   1868742|         5489|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.18.02|ceddotalk|ceddo|b...|     78050|           95|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.20.02|ceddotalk|ceddo|b...|    210267|          203|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.21.02|ceddotalk|ceddo|b...|    255876|          231|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|60 Sekunden Class...|2024-02-20 10:42:59|          Bundeswehr|     24.21.02|Jaguar 1|Jagdpanz...|    281838|           98|News & Politics|          0|           5.0|(12,[5],[1.0])|(14,[5,12,13],[1....|\n",
      "|90+10.! Entscheid...|2024-02-18 17:34:31|          SPORTTOTAL|     24.19.02|sporttotal.tv|spo...|     46958|          255|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|90+10.! Entscheid...|2024-02-18 17:34:31|          SPORTTOTAL|     24.20.02|sporttotal.tv|spo...|     61109|          273|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+--------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 10:41:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/02/22 10:41:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Accuracy: 0.9827726690722849\n",
      "Model: DecisionTreeClassifier, Accuracy: 1.0\n",
      "Model: RandomForestClassifier, Accuracy: 0.9988359911535328\n",
      "Model: GBTClassifier, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "##########################\n",
    "# Initialize the models\n",
    "##########################\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='is_trending')\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='is_trending')\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='is_trending')\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='is_trending')\n",
    "\n",
    "\n",
    "models = [lr, dt, rf, gbt]\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_trending\", metricName=\"areaUnderROC\")\n",
    "\n",
    "################################\n",
    "# Train and evaluate each model\n",
    "################################\n",
    "for model in models:\n",
    "    # Train model\n",
    "    model_fit = model.fit(train_data)  \n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model_fit.transform(test_data)  \n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Model: {model.__class__.__name__}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+--------------+--------------+--------------------+\n",
      "|               title|        publishedAt|        channelTitle|trending_date|                tags|view_count|comment_count|  categoryTitle|is_trending|category_index|  category_vec|            features|\n",
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+--------------+--------------+--------------------+\n",
      "|1. FC Heidenheim ...|2024-02-19 00:00:16| sportstudio fußball|     24.20.02|Fußball|Fussball|...|    460727|          319|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|1. FC Heidenheim:...|2024-02-19 16:00:08|         Manu Thiele|     24.21.02|Fußball|Fußball N...|     88738|          493|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|1. FC Heidenheim:...|2024-02-19 16:00:08|         Manu Thiele|     24.21.02|Fußball|Fußball N...|     91215|          501|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|10 Minute Evening...|2024-02-18 08:00:15|       Mady Morrison|     24.18.02|mady morrison|str...|     28422|          109|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|10 Minute Evening...|2024-02-18 08:00:15|       Mady Morrison|     24.21.02|mady morrison|str...|    132518|          172|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|10 Things Conor M...|2024-02-14 18:00:04|           GQ Sports|     24.20.02|10 essentials|bla...|   1757681|         2568|         Sports|          1|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|17 Minuten extrem...|2024-02-18 12:00:26|             zCrexpy|     24.21.02|papaplatte|papapl...|    156535|           90|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|24 Stunden Fußbal...|2024-02-15 15:00:42|Das schaffst du nie!|     24.18.02|Das schaffst du n...|    235669|          930|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|24 Stunden Fußbal...|2024-02-15 15:00:42|Das schaffst du nie!|     24.21.02|Das schaffst du n...|    271270|          992|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|5. Mio € Deal & M...|2024-02-18 20:00:38|                Alim|     24.20.02|              [none]|    188571|          526|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.18.02|versus 1|versus 1...|   1446676|         4713|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.20.02|versus 1|versus 1...|   1745302|         5299|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.21.02|versus 1|versus 1...|   1849125|         5461|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|50 singles speed ...|2024-02-14 18:00:51|              nectar|     24.21.02|versus 1|versus 1...|   1868742|         5489|  Entertainment|          1|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.18.02|ceddotalk|ceddo|b...|     78050|           95|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.20.02|ceddotalk|ceddo|b...|    210267|          203|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|6 MÄDELS BLIND DA...|2024-02-18 12:00:00|               CEDDO|     24.21.02|ceddotalk|ceddo|b...|    255876|          231|  Entertainment|          0|           0.0|(12,[0],[1.0])|(14,[0,12,13],[1....|\n",
      "|60 Sekunden Class...|2024-02-20 10:42:59|          Bundeswehr|     24.21.02|Jaguar 1|Jagdpanz...|    281838|           98|News & Politics|          0|           5.0|(12,[5],[1.0])|(14,[5,12,13],[1....|\n",
      "|90+10.! Entscheid...|2024-02-18 17:34:31|          SPORTTOTAL|     24.19.02|sporttotal.tv|spo...|     46958|          255|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "|90+10.! Entscheid...|2024-02-18 17:34:31|          SPORTTOTAL|     24.20.02|sporttotal.tv|spo...|     61109|          273|         Sports|          0|           1.0|(12,[1],[1.0])|(14,[1,12,13],[1....|\n",
      "+--------------------+-------------------+--------------------+-------------+--------------------+----------+-------------+---------------+-----------+--------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing on Fake Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Step 1: Create a new DataFrame with the required structure\n",
    "new_data = [\n",
    "    {\"categoryTitle\": \"Music\",\"title\": \"New Video 1\", \n",
    "      \"view_count\": 100000, \"comment_count\": 500,\n",
    "      \"publishedAt\": \"2024-02-18T11:00:08Z\",\"channelTitle\" : \"h\",\"tags\": \"ggg\", \"trending_date\" : \"24.18.02\"}\n",
    "    \n",
    "]\n",
    "new_df = spark.createDataFrame(new_data)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"categoryTitle\", outputCol=\"category_index\").fit(df)\n",
    "encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_vec\")\n",
    "assembler = VectorAssembler(inputCols=[\"category_vec\", \"view_count\", \"comment_count\"], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[indexer,encoder,assembler])\n",
    "pipline_model = pipeline.fit(df)\n",
    "new_df_transformed = pipline_model.transform(new_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- categoryTitle: string (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- comment_count: long (nullable = true)\n",
      " |-- publishedAt: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- view_count: long (nullable = true)\n",
      " |-- category_index: double (nullable = false)\n",
      " |-- category_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- publishedAt: timestamp (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- categoryTitle: string (nullable = true)\n",
      " |-- is_trending: integer (nullable = false)\n",
      " |-- category_index: double (nullable = false)\n",
      " |-- category_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_transformed.printSchema()\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- categoryTitle: string (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- comment_count: long (nullable = true)\n",
      " |-- publishedAt: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- view_count: long (nullable = true)\n",
      " |-- category_index: double (nullable = false)\n",
      " |-- category_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+-------------+--------------------+----------+\n",
      "|      title|categoryTitle|view_count|comment_count|            features|prediction|\n",
      "+-----------+-------------+----------+-------------+--------------------+----------+\n",
      "|New Video 1|        Music|    100000|          500|(14,[2,12,13],[1....|       0.0|\n",
      "+-----------+-------------+----------+-------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = bestModel.transform(new_df_transformed)\n",
    "\n",
    "# Show the predictions\n",
    "predictions.select(\"title\", \"categoryTitle\", \"view_count\", \"comment_count\", \"features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonMlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
